---
title: "R Notebook"
output: html_notebook
---

Human Resources Analytics: Kaggle

Why are our best and most experienced employees leaving prematurely? Have fun with this database and try to predict which valuable employees will leave next. Fields in the dataset include:

Employee satisfaction level

Last evaluation

Number of projects

Average monthly hours

Time spent at the company

Whether they have had a work accident

Whether they have had a promotion in the last 5 years

Sales

Salary

Whether the employee has left

```{r}
library(dplyr)
library(tidyr)
library(ggplot2)
library(ggvis)
library(corrplot)
library(DT)
library(readr)
suppressMessages(alldata <-read_csv("HR_comma_sep.csv"))

```
```{r}
dim(alldata)
```
```{r}
summary(alldata)
```
We see from the above summary that 24% of the employees in the data left the company. The average level of satisfaction is 62%, average number of projects worked on was 3.8, average monthly hours of work is 201, average time spent in the company is 3.5 years and the average number of promotions in the last 5 years has been 0.02.

Let us now look at the correlations between our variables:
```{r}
HR_correlation <- alldata %>% select(satisfaction_level:promotion_last_5years)
M <- cor(HR_correlation)
corrplot(M, method="circle")
```
We see that reported job satisfaction level has a significant inverse correlation with leaving. That is, those of low reported job satisfaction were likely to leave. We also see that there was small correlation between having spent much time in the company and not leaving. Those who have had a work accident were more likely to leave as well. Interestingly, having been promoted in the last 5 years did not correlation with likelihood of leaving.

Let's consider now only those who leave:
```{r}
leavers <- alldata %>% filter(left==1)
nrow(leavers)
```
and let's see what features they had

```{r}
par(mfrow=c(1,3))

hist(leavers$satisfaction_level,col="#3090C7", main = "Satisfaction level") 

hist(leavers$last_evaluation,col="#3090C7", main = "Last evaluation")

hist(leavers$time_spend_company,col="#3090C7", main = "Time spent in the company")

```
Looking at the "Last evaluation" graph, we see an interesting distribution. It seems that those who who received a poor evaluation and those who scored highly on the evaluation were likely to leave. Those who were in-between, were likely to stay.
However, we need to see whether the dstribution of evaluations was uniform. If there are very few lukewarm evaluations, then we can't conclude much.

```{r}
par(mfrow=c(1,3))
hist(alldata$satisfaction_level,col="#3090C7", main = "Satisfaction level") 
hist(alldata$last_evaluation,col="#3090C7", main = "Last evaluation")
hist(alldata$time_spend_company,col="#3090C7", main = "Time spent in the company")
```
Now we are much more confident in saying that "mediocre" employees will stay, while bad ones and good ones will leave.
Of course we also see that those who leave are statistically less satisfied.

The number of leavers is

```{r}
nrow(leavers)
```
Let's look at employees that should have been retained. These are the ones that either received a high evaluation or worked on many projects at once.

```{r}
good_leavers <- leavers %>% filter(last_evaluation >= 0.75 | number_project >= 5)
nrow(good_leavers)
```
This turns out to be the majority of employees that left the company. So there is indeed a potential for improvement when it comes to retaining the desirable employees.

Next, we will build a predictive model for which employees will leave next. 

I'll add a 0-1 column that specifies whether a worker is a "good leaver" or not and remove the column for left.

```{r}
alldata$goodleft <- 1*((alldata$last_evaluation>=0.75 | alldata$number_project>=5) & alldata$left==1)
alldata$left <- NULL
head(alldata)

```


```{r}
library("caret")
split=0.80
trainIndex <- createDataPartition(alldata$goodleft, p=split, list=FALSE)
train <- alldata[ trainIndex,]
test <- alldata[-trainIndex,]
```



```{r}
model <- glm (goodleft ~ ., data = train, family = binomial)
summary(model)
```
```{r}
predict <- predict(model, type = 'response')
confusion_train=table(train$goodleft, predict > 0.5)
# accuracy on test data
(confusion_train[1,1]+confusion_train[2,2])/nrow(train)

```
```{r}
prediction<-(predict.glm(model, test[,-10],type='response')>0.5)*1
# accuracy on test data
sum((prediction==test[,10])*1)/nrow(test)
```
It looks like logistic regression does a very good job on the data. We can now predict which good workers are likely to leave.
It is interesting to look at the coefficients in the regression to get a sense of what factors can predict that a worker is both good and likely to leave:

```{r}
coefficients(model)
```

